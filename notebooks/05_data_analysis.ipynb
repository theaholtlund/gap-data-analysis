{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing Retrieved Data\n",
    "\n",
    "This Jupyter Notebook is for examination and subsequent validation of the data generated by the previous data collection notebooks. The real framework value from a management perspective lies in analysing, comparing and constrasting the data, automating the work of pointing out findings that may be of significance in the redistribution process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules and libraries\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from ydata_profiling import ProfileReport\n",
    "from packaging import version\n",
    "from packaging.version import Version, parse\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Get current working directory and append parent directory for module imports\n",
    "cwd = os.getcwd()\n",
    "parent_dir = os.path.dirname(cwd)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Import modules from other project scripts\n",
    "from data_constants import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing GitHub API Calls\n",
    "\n",
    "Connecting to GitHub and verifying the user token is completed by storing the access token as an environment variable. The function for getting the token is imported from the utils file in the project. As the API has a limit of 5000 calls per hour, the remaining calls available and the reset time is tracked below. Each function to retrieve and export the data in the data retrieval files have functionality to pause the workflow when the user runs out of API calls. The program will automatically sleep until the limit is renewed, and then resume the job it was completing at the time the limit ran out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the total and remaining GitHub API calls, and see when the limit will reset\n",
    "remaining_requests, request_limit = g.rate_limiting\n",
    "print(f\"Request limit for API Calls: {request_limit}\")\n",
    "print(f\"Remaining requests for API Calls: {remaining_requests}\")\n",
    "\n",
    "limit_reset_time = g.rate_limiting_resettime\n",
    "reset_time = datetime.fromtimestamp(limit_reset_time).strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(f\"Reset time for API Calls: {reset_time}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing and Evaluating the Data\n",
    "\n",
    "As this framework intends to separate code and data, the validation process will use the data generated by the user in the data extraction process as the basis for analysis. No information is provided except from the created by the user in the 'collected_data'folder. The focus of the analysis and validation process is to provide an executive summary on the data, while pointing to any outliers and findings of significance. As such, this notebook highlights relationships and deviations that could be of importance to the redustribution process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions to Analyse Retrieved Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_versions(data: dict) -> tuple:\n",
    "    \"\"\"Check the versions provided in the data.\n",
    "\n",
    "    Args:\n",
    "        data (dict): The data containing package versions.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - ci_only_packages (list): Packages with versions specified only in the CI file.\n",
    "            - package_info_only_packages (list): Packages with versions specified only in the PackageInfo file.\n",
    "            - both_versions_packages (list): Packages with versions specified in both CI file and PackageInfo file.\n",
    "    \"\"\"\n",
    "    ci_only_packages = []\n",
    "    package_info_only_packages = []\n",
    "    both_versions_packages = []\n",
    "\n",
    "    for package, versions in data.items():\n",
    "        ci_versions = versions.get('tested_ci_versions', [])\n",
    "        package_info_versions = versions.get('required_pkginfo_version', [])\n",
    "\n",
    "        if ci_versions and not package_info_versions:\n",
    "            ci_only_packages.append(package)\n",
    "\n",
    "        elif package_info_versions and not ci_versions:\n",
    "            package_info_only_packages.append(package)\n",
    "\n",
    "        elif ci_versions and package_info_versions:\n",
    "            both_versions_packages.append(package)\n",
    "\n",
    "    return ci_only_packages, package_info_only_packages, both_versions_packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_ci_and_pkg_versions(data: dict) -> list:\n",
    "    \"\"\"Compare the CI and PackageInfo data to see if all versions tested in the \n",
    "    CI file are equal to or greater than the required version from the PackageInfo file.\n",
    "\n",
    "    Args:\n",
    "        data (dict): The data containing package versions.\n",
    "\n",
    "    Returns:\n",
    "        list: Packages where not all versions in the CI file are above that in the PackageInfo file.\n",
    "    \"\"\"\n",
    "    packages_with_mismatch = []\n",
    "\n",
    "    for package, versions in data.items():\n",
    "        ci_versions = versions.get(\"tested_ci_versions\", [])\n",
    "        package_version = versions.get(\"required_pkginfo_version\")\n",
    "\n",
    "        if ci_versions and package_version:\n",
    "            if not all(version.parse(ci) >= version.parse(package_version[0]) for ci in ci_versions):\n",
    "                packages_with_mismatch.append(package)\n",
    "\n",
    "    return packages_with_mismatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_next_version(version: Version) -> Version:\n",
    "    \"\"\"Find the next version after the given version, to check for gaps in testing patterns.\n",
    "\n",
    "    Args:\n",
    "        version (Version): The version for which to find the next version.\n",
    "\n",
    "    Returns:\n",
    "        Version: The next version that should be after the given version.\n",
    "    \"\"\"\n",
    "    version_info = list(version.release)\n",
    "    if version_info[-1] == 0:\n",
    "        version_info[-2] = version_info[-2] + 1\n",
    "    else:\n",
    "        version_info[-1] = version_info[-1] + 1\n",
    "    return Version(\".\".join(str(comp) for comp in version_info))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def version_tuple(version_str: str) -> tuple:\n",
    "    \"\"\"Convert a version string to a tuple representation.\n",
    "\n",
    "    Args:\n",
    "        version_str (str): The version string to be converted.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple representation of the version, consisting of\n",
    "        major version, minor version and patch version.\n",
    "    \"\"\"\n",
    "    version = parse(version_str)\n",
    "    version_info = list(version.release)\n",
    "    if len(version_info) == 2:\n",
    "        version_info.append(0)\n",
    "    return tuple(version_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_version_gaps(data: dict) -> dict:\n",
    "    \"\"\"Check for gaps in version testing between the tested versions and the required version,\n",
    "    discarding patch components as gaps. This is done by checking if the lowest tested version \n",
    "    is exactly 0.1 higher than the required version.\n",
    "\n",
    "    Args:\n",
    "        data (dict): The data containing package versions.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dict mapping package names to True if there are version gaps, False otherwise.\n",
    "    \"\"\"\n",
    "    version_gaps = {}\n",
    "    for package, package_data in data.items():\n",
    "        required_versions = package_data.get(\"required_pkginfo_version\", [])\n",
    "        if not required_versions:\n",
    "            continue\n",
    "\n",
    "        ci_versions = package_data.get(\"tested_ci_versions\", [])\n",
    "        if not ci_versions:\n",
    "            continue\n",
    "\n",
    "        required_version_str = required_versions[0]\n",
    "        required_version = version_tuple(required_version_str)\n",
    "        lowest_ci_version = min(version_tuple(v) for v in ci_versions)\n",
    "\n",
    "        if (\n",
    "            lowest_ci_version[:-2] == required_version[:-2]\n",
    "            and lowest_ci_version[-2] == required_version[-2] + 1\n",
    "            and lowest_ci_version[-1] == 0\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        next_version = find_next_version(parse(required_version_str))\n",
    "\n",
    "        has_gap = not (\n",
    "            (lowest_ci_version[:-1] >= required_version[:-1] and lowest_ci_version <= version_tuple(str(next_version)))\n",
    "            or (lowest_ci_version[:-2] == required_version[:-2] and lowest_ci_version[-1] == required_version[-1] + 1)\n",
    "        )\n",
    "\n",
    "        if has_gap:\n",
    "            version_gaps[package] = has_gap\n",
    "\n",
    "    return version_gaps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyse and Display Extracted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define organisation and repositories\n",
    "org = g.get_organization(ORG_NAME_PACKAGES)\n",
    "repos = org.get_repos(type=\"public\")\n",
    "\n",
    "# Load the repository data from the JSON file\n",
    "data_folder = \"collected_data\"\n",
    "repo_file_path = os.path.join(data_folder, \"repo_data.json\")\n",
    "repo_data = load_data(repo_file_path)\n",
    "\n",
    "# Load monitoring data from the JSON file\n",
    "monitoring_file_path = os.path.join(data_folder, \"monitoring_data.json\")\n",
    "monitoring_data = load_data(monitoring_file_path)\n",
    "\n",
    "# Load testing data from the JSON file\n",
    "testing_file_path = os.path.join(data_folder, \"testing_data.json\")\n",
    "testing_data = load_data(testing_file_path)\n",
    "\n",
    "# Load community data from the JSON file\n",
    "community_file_path = os.path.join(data_folder, \"community_data.json\")\n",
    "community_data = load_data(community_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### General Statistics: GAP Packages and Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of GAP repositories hosted by the GAP organisation on GitHub\n",
    "total_packages = repos.totalCount\n",
    "print(f\"Number of GAP packages from GAP Respository: {total_packages}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the latest release and version of GAP\n",
    "repo_url = f\"https://api.github.com/repos/{ORG_NAME_SYSTEM}/gap/releases/latest\"\n",
    "headers = {'Authorization': f'token {github_token}'}\n",
    "response = requests.get(repo_url, headers=headers)\n",
    "latest_release = response.json()\n",
    "latest_version = latest_release.get(\"tag_name\")\n",
    "print(f\"The latest version of GAP is: {latest_version}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of GAP packages hosted elsewhere on GitHub\n",
    "# The information is attempted gathered through the web scraping technique provided by Beautiful Soup\n",
    "# NB: These numbers are only indicative and not completely accurate due to the webpage listing style, counts per parent list item\n",
    "url = \"https://gap-packages.github.io/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Find the section of the webpage with the packages stored elsewhere on GitHub\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "section = soup.find(\"section\", id=\"main-content\")\n",
    "ul = section.find_next(\"ul\")\n",
    "\n",
    "# Do not include any child elements that are ul or li, as not to let these increase the count\n",
    "packages = ul.find_all(\"li\", recursive=False)\n",
    "count = len(packages)\n",
    "\n",
    "print(f\"Number of GAP packages hosted elsewhere on GitHub: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Repository Data: Key Metrics and Notable Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall statistics and generate relevant insights for each repository\n",
    "total_repos = len(repo_data)\n",
    "total_releases = sum(repo['total_releases'] for repo in repo_data)\n",
    "total_open_issues = sum(repo['open_issues_count'] for repo in repo_data)\n",
    "total_open_pull_requests = sum(repo['open_pull_requests'] for repo in repo_data if repo['open_pull_requests'])\n",
    "total_bug_count = sum(repo['bug_count'] for repo in repo_data if repo['bug_count'])\n",
    "total_enhancement_count = sum(repo['enhancement_count'] for repo in repo_data if repo['enhancement_count'])\n",
    "\n",
    "# Display the calculated overall metrics\n",
    "print(f\"Total Repositories: {total_repos}\")\n",
    "print(f\"Total Releases: {total_releases}\")\n",
    "print(f\"Total Open Issues: {total_open_issues}\")\n",
    "print(f\"Total Open Pull Requests: {total_open_pull_requests}\")\n",
    "print(f\"Total Bug Count: {total_bug_count}\")\n",
    "print(f\"Total Enhancement Count: {total_enhancement_count}\")\n",
    "\n",
    "# Display inactive repositories where there has been no activity in the last 90 days\n",
    "pd.set_option('display.max_rows', None)\n",
    "inactive_repositories = [repo['repo'] for repo in repo_data if repo['last_activity_time'] is None]\n",
    "\n",
    "if inactive_repositories:\n",
    "    # Create a DataFrame from the list of inactive repositories\n",
    "    df = pd.DataFrame(inactive_repositories, columns=['Inactive Repositories'])\n",
    "    display(df)\n",
    "else:\n",
    "    print(\"All repositories had activity within in the past 90 days.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate statistical analysis for other variables, using a ProfileReport\n",
    "data_list = [\n",
    "    {\n",
    "        'total_releases': repo['total_releases'],\n",
    "        'age_in_days': repo['age_in_days'],\n",
    "        'open_issues_count': repo['open_issues_count'],\n",
    "        'total_pull_requests': repo['total_pull_requests'],\n",
    "        'open_pull_requests': repo['open_pull_requests'],\n",
    "        'closed_pull_requests': repo['closed_pull_requests'],\n",
    "    }\n",
    "    for repo in repo_data\n",
    "]\n",
    "\n",
    "# Create a DataFrame from the list of dicts\n",
    "repo_df = pd.DataFrame(data_list)\n",
    "\n",
    "# Generate the ProfileReport based on selected columns\n",
    "profile = ProfileReport(repo_df, title=\"Statistical analysis for repositories managed by gap-system organisation on GitHub\")\n",
    "profile.to_widgets()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Monitoring Data: Key Metrics and Notable Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the relevant information from the loaded data\n",
    "packages_with_different_versions = monitoring_data['packages_with_different_versions']\n",
    "all_previous_and_maybe_next = monitoring_data['all_previous_and_maybe_next']\n",
    "previous_and_maybe_next_labels = monitoring_data['previous_and_maybe_next_labels']\n",
    "\n",
    "# Compare the latest released version number to the one on the main branch in GAP PackageDistro\n",
    "# Packages with different versions numbers will be in the next GAP release\n",
    "df_versions = pd.DataFrame(packages_with_different_versions)\n",
    "df_versions = df_versions[['package_name', 'latest_version', 'main_branch_version']]\n",
    "print(\"Packages with different versions in the latest GAP release and in the GAP PackageDistro: \")\n",
    "display(df_versions)\n",
    "\n",
    "# Find the packages in unmerged pull requests, as these may be in the next release but have not yet been merged\n",
    "df_previous_next = pd.DataFrame(all_previous_and_maybe_next, columns=['Package'])\n",
    "print(\"\\nAll packages that were in the previous release and have unmerged pull requests: \")\n",
    "display(df_previous_next)\n",
    "\n",
    "# Only retrieve packages with unmerged pull requests that have certain labels indicating release relation\n",
    "df_labels = pd.DataFrame(previous_and_maybe_next_labels, columns=['Package'])\n",
    "print(\"\\nPackages with release related labels that were in the previous release and have unmerged pull requests: \")\n",
    "display(df_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing Data: Key Metrics and Notable Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of test directories and test files for all the repositories\n",
    "tst_dirs_with_files = len(testing_data)\n",
    "total_test_files = sum(data.get(\"tst_file_count\", 0) for data in testing_data.values())\n",
    "display(Markdown(f\"**Repositories with test directories containing files:** {tst_dirs_with_files}\"))\n",
    "display(Markdown(f\"**Total number of test files for all packages:** {total_test_files}\"))\n",
    "\n",
    "# Get the number of repositories with a CI file, and the names of the ones that does not have one\n",
    "repos_with_ci_file = [package for package, data in testing_data.items() if \"tested_ci_versions\" in data]\n",
    "num_packages_without_tests = len([package for package, data in testing_data.items() if \"tested_ci_versions\" not in data])\n",
    "display(Markdown(f\"**Number of repositories with CI file:** {len(repos_with_ci_file)}\"))\n",
    "\n",
    "# Get the number of repositories with a PackageInfo file, and the names of the ones who does not have one\n",
    "repos_with_pkginfo_file = [package for package, data in testing_data.items() if \"required_pkginfo_version\" in data]\n",
    "repos_without_pkginfo_file = [package for package in testing_data.keys() if package not in repos_with_pkginfo_file]\n",
    "display(Markdown(f\"**Number of repositories with PackageInfo.g file:** {len(repos_with_pkginfo_file)}\"))\n",
    "\n",
    "# Compare version testing in tested versions and required version for packages\n",
    "ci_only_packages, package_info_only_packages, both_versions_packages = check_versions(testing_data)\n",
    "display(Markdown(f\"**Number of packages with CI version testing but no required version:** {len(ci_only_packages)}\"))\n",
    "display(Markdown(f\"**Number of packages with required version but no CI version testing:** {len(package_info_only_packages)}\"))\n",
    "display(Markdown(f\"**Number of packages with both CI version testing and required version:** {len(both_versions_packages)}\"))\n",
    "\n",
    "# Create a DataFrame with more detailed information on the testing for  each repository\n",
    "df_detailed_info = pd.DataFrame({\n",
    "    \"Repository\": list(testing_data.keys()),\n",
    "    \"Count Test Files\": [data.get(\"tst_file_count\", 0) for data in testing_data.values()],\n",
    "    \"Tested CI Versions\": [', '.join(data.get(\"tested_ci_versions\", [\"None\"])) for data in testing_data.values()],\n",
    "    \"Required PkgInfo Version\": [', '.join(data.get(\"required_pkginfo_version\", [\"None\"])) for data in testing_data.values()],\n",
    "    \"CI file has test data in file\": ['Yes' if \"tested_ci_versions\" in data else 'No' for package, data in testing_data.items()],\n",
    "    \"Package has PackageInfo file\": ['Yes' if package in repos_with_pkginfo_file else 'No' for package in testing_data.keys()]\n",
    "})\n",
    "\n",
    "display(df_detailed_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the repositories with the latest version of GAP in their tested versions from the CI file\n",
    "# Remove any prefix or additional text after the version number, if provided for the current version, do not account for patch versions\n",
    "latest_package_version = latest_version.lstrip(\"v\")\n",
    "latest_package_version = \".\".join(latest_package_version.split(\".\")[:2])\n",
    "\n",
    "latest_version_obj = version.parse(latest_package_version)\n",
    "repos_with_latest_gap_version = [\n",
    "    package for package, data in testing_data.items() if any(latest_version_obj == version.parse(ci) for ci in data.get(\"tested_ci_versions\", []))\n",
    "]\n",
    "\n",
    "count_repos_with_latest_gap_version = len(repos_with_latest_gap_version)\n",
    "print(f\"Number of repositories with the latest version of GAP in their tested versions: {count_repos_with_latest_gap_version}\")\n",
    "\n",
    "if count_repos_with_latest_gap_version > 0:\n",
    "    df_repos = pd.DataFrame(repos_with_latest_gap_version, columns=['Package'])\n",
    "    print(\"\\nRepositories with the latest version of GAP in their tested versions:\")\n",
    "    display(df_repos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if package tested versions are equal to or greater than the required version\n",
    "packages_with_mismatch = compare_ci_and_pkg_versions(testing_data)\n",
    "df_packages_with_mismatch = pd.DataFrame(packages_with_mismatch, columns=['Package'])\n",
    "\n",
    "if not df_packages_with_mismatch.empty:\n",
    "    print(\"Tested versions are not all greater than or equal to required version for the following packages:\")\n",
    "    display(df_packages_with_mismatch)\n",
    "else:\n",
    "    print(\"All packages have tested versions that are greater than or equal to the required version.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for gaps in the tested versions and the required version for each package, or higher required than some tested version\n",
    "version_gaps = check_version_gaps(testing_data)\n",
    "df_version_gaps = pd.DataFrame([(package, ', '.join(data.get(\"tested_ci_versions\", [\"None\"])), ', '.join(data.get(\"required_pkginfo_version\", [\"None\"]))) for package, data in testing_data.items() if version_gaps.get(package)], columns=['Package', 'CI File Version', 'Required PkgInfo Version'])\n",
    "\n",
    "if not df_version_gaps.empty:\n",
    "    print(\"Packages with gaps in tested and required version, or where required version is higher than some tested version: \")\n",
    "    display(df_version_gaps)\n",
    "else:\n",
    "    print(\"No packages with gaps in tested and required version, or higher required version than some tested version.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Community Data: Key Metrics and Notable Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get key numbers for contributors of GAP on GitHub\n",
    "# Currently, the inactive contributors checks for contributors with no commits for the past 12 months\n",
    "all_authors = community_data['authors']\n",
    "all_submitters = community_data['submitters']\n",
    "author_submitters = community_data['author_submitters']\n",
    "inactive_contributors = community_data['inactive_contributors']\n",
    "\n",
    "print(f\"Total number of authors for all GAP packages: {len(all_authors)}\")\n",
    "print(f\"Total number of submitters for all GAP packages: {len(all_submitters)}\")\n",
    "print(f\"Total number of authors who were also submitters for all GAP packages: {len(author_submitters)}\")\n",
    "print(f\"Total number of inactive contributors for all GAP packages: {len(inactive_contributors)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save Notebook For Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the script has been executed once and the outputs have been generated, it is very important to **save the file** before starting the Streamlit dashboard. If not, the outputs will not be available on the dashboard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
